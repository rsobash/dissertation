\chapter{Background}

This chapter will focus on background material relevant to the experiments performed in the following chapters. A brief review of the sources of forecast errors and ensemble forecasting methods are provided in Section 2.1. In Section 2.2, data assimilation methods will be summarized, while Sections 2.3, 2.4, and 2.5 will summarize studies that have employed these data assimilation techniques on the convective and mesoscales, using radar and surface observations. 

\section{NWP errors and predictability}
Creating an accurate three-dimensional estimate of the current atmospheric state is paramount to producing accurate weather forecasts of the future state. \citet{lorenz63}, using a low-order model of convection, observed that a slight perturbation to the initial conditions, originating from rounding error, would lead to deviations in the solution to the equations that would amplify with time. Lorenz suggested this instability would impose limits on atmospheric predictability, since it is impossible to completely describe the current state of the atmosphere (even in the limit of an infinitely dense, perfect, observing network, there remain scales that cannot be observed with complete accuracy). \citep{lorenz69} quantified the effects of these unobserved scales as they feedback to the larger scales over time, deteriorating the forecast; this cascade of errors imposes an {\it intrinsic} predictability limit on a numerical weather prediction (NWP) forecast. In addition to the intrinsic predictability limitations, observations and the methods used to produce the initial conditions both possess errors; these types of initial conditions errors impose a {\it practical} predictability limit on NWP forecasts \cite{melhauserzhang12}.

Additional practical predictability limitations are imposed by NWP model error. NWP models are composed of partial differential equations that describe the fundamental atmospheric processes and their time evolution (e.g. advection, thermodynamics). These equations are discretized onto a three-dimensional grid, and are integrated in time to produce predictions of the future state, given a set of initial and boundary conditions. Using the discretized approximation to the equation set introduces error, as scales below a given threshold are not represented, but is needed since no analytical solution to the equations is known to exist. The discretization process leads to physical processes that are not represented on the model grid. These physical processes need to be parameterized to incorporate their effects on the resolved scales (e.g. boundary layer mixing, precipitation production), and this parameterization leads to further errors.

NWP accuracy is hindered by the magnitude of these initial condition errors and model errors; reducing one or both of these error sources can improve forecast skill and increase predictability \citep{lorenz65}. The precise sensitivity that forecasts can be flow dependent; large errors can be tolerated in certain atmospheric flow patterns, while small errors in other patterns can lead to rapid forecast degradation \citep{lorenz65,zhangetal06b}. Further, errors grow more rapidly on smaller scales (convection-scale and mesoscale) than larger scales (synoptic and global scales), leading to a shorter limit of predictability on smaller time scales (e.g. \citealt{zhangetal03}).

Atmospheric predictability limitations have led to the development of ensemble forecasting methods \citep{leith74,tractonkalnay93}. An ensemble forecast can provide a best solution (i.e. ensemble mean) and an estimate of the atmospheric predictability (i.e. ensemble variance). In an ensemble system, a set of initial states is generated that encompasses the range of uncertainty in the initial state and attempts to capture the most unstable modes of error growth \citep{tothkalnay93}. Much effort has gone into developing methods that produce an appropriate range of initial atmospheric states within the ensemble (e.g. \citealt{leith74,hoffmankalnay83,tothkalnay93,palmeretal92}). Given the rapid error growth rates associated with moist convection \citep{zhangetal06b}, ensembles are necessary on the convective-scale. Recent increases in computing power have permitted convective-scale ensemble forecasts that have provided skillful predictions of convective precipitation and severe weather (e.g. \citealt{kongetal06,clarketal09,clarketal10}).

Advanced data assimilation techniques, combined with ensemble forecasting, are both ways to extend practical atmospheric predictability. Discussion of contemporary data assimilation techniques follow in the next section.

\pagebreak
\section{Data assimilation}
Data assimilation (DA; often referred to as state estimation) is used to generate initial conditions for NWP models. DA is the process of combining many sources of information about the current atmospheric state to produce an analysis. Information most commonly originates from observations and a “first-guess” atmospheric state, often a climatological state estimate or one generated from a NWP model forecast in a previous analysis cycle. Various DA techniques will be reviewed below, with particular emphasis on those employed in convective-scale DA studies.

\subsection{Optimal interpolation and 3DVAR}
\citet{gandin63} originally derived the multivariate optimal interpolation (OI) equations that are based on statistical estimation theory. A full derivation of the OI equations can be found in various texts, including \citet{kalnaybook} and \citet{varahanbook}, and are stated below without derivation for brevity. The equations are cast in matrix form, with uppercase letters denoting matrices and lowercase letters denoting vectors.

\begin{equation}
   x^a = x^b + W(y^o - H(x^b)) = x^b + Wd
\label{OI_1}
\end{equation}
\begin{equation}
\label{OI_2}
   W = BH^T(HBH^T + R)^{-1}
\end{equation}

OI produces an {\it optimal} analysis  \(x^a\), by adjusting the background state, \(x^b\), by weighted observation innovations, \(Wd\) (Equation \ref{OI_1}). The adjustments are made to \(x^b\) at the locations of the observations, thus the observations \(y^o\) need to be compared to \(x^b\) using a forward model, \(H\), that maps \(x^b\) into observation space. The weights, \(W\), are computed to minimize the analysis error variance using \ref{OI_2}. The optimal weights produce an analysis that has less uncertainty (or at the very least, the same uncertainty) than either the uncertainty associated with \(y^o\) or \(x^b\). Computing \(W\) is dependent on statistical information that describes the errors associated with the observations and background, these statistics are contained within the \(R\) and \(B\) matrices, respectively.

In variational approaches, including three-dimensional variational assimilation (3DVAR), a scalar cost function is formulated that is composed of terms that represent the departure between an analysis and various pieces of information \citep{sasaki70}. An example cost function, similar to that used in \citet{gaoetal04}, is shown below:

\begin{equation}
\label{3DVAR}
   J(x) = J_b + J_o + J_c
\end{equation}
\begin{equation}
\label{3DVAR2}
   J(x) = \frac{1}{2}(x - x^b)^TB^{-1}(x - x^b) + \frac{1}{2}(H(x) - y_o)^TR^{-1}(H(x) - y_o) + J_c
\end{equation}

Terms in the cost function can include the departure between the analysis and observations, \(J_o\), background, \(J_b\), and dynamic or smoothness constraints, \(J_c\), (e.g. \citealt{gaoetal04,geetal12}; equation \ref{3DVAR}). \(J_b\) and \(J_o\) are weighted inversely by \(R\) and \(B\), respectively (equation \ref{3DVAR2}). The final analysis is  \(x^a\) is chosen as the \(x\) which minimizes the cost function. This occurs where the derivative of J becomes zero.

While OI and 3DVAR produce analyses in different ways (OI computes optimal weights that minimize the analysis error variance, while 3DVAR computes an analysis that minimizes a cost function), \citet{lorenc86} demonstrated the equivalency of the two techniques. Accurately representing \(R\) and \(B\) remains a significant challenge for both approaches. \(B\) is typically represented by a static matrix containing isotropic error covariance structures. Thus, the background errors are estimated once and assumed to be stationary. This neglects the forecast "errors of the day" and, as will be discussed in section \ref{section_kf}, is insufficient for the meso- and convective-scales where error covariance structures can be flow-dependent.  \(R\) is usually constructed as a diagonal matrix, assuming that observation errors are uncorrelated.

\subsection{4DVAR}
Four-dimensional variational assimilation (4DVAR) is an extension of 3DVAR. In 4DVAR, the \(J_o\) term in the cost function is amended to include observations within an assimilation window, rather than at a single time \citep{talagrandcourtier87}. The model is integrated forward to compute the discrepancies between the observations and the analysis at each observation time. 4DVAR produces an initial condition and forecast trajectory that best fits the observations within the assimilation window.

Since the forward model is used to compute the observation discrepancies in the cost function, the model is assumed to be perfect in 4DVAR. Other assumptions include the use of a tangent-linear and adjoint model to simplify the minimization of the cost function. Further, simplified physics are used within these linearized models. Using the linearized tangent-linear model can be problematic with long assimilation windows and when simulating complex physical processes \citep{tremolet04}.  Even with these assumptions, 4DVAR excels over 3DVAR and OI, both due to its ability to evolve the background error covariance matrix, \(B\) and to compare each observation to an analysis state valid at the time it was observed \citep{lorencrawlins05}. The flow-dependent background error is a significant advantage over OI and 3DVAR, which use static estimates of \(B\). In 4DVAR, the new analysis error covariance matrix is unavailable at the end of the assimilation window, thus each 4DVAR cycle must begin with a new estimate for \(B\).

\subsection{The Kalman filter}
\label{section_kf}
The DA techniques used herein are derived from the Kalman filter (KF; \citealt{kalman60,kalmanbucy61}). The objective of the KF and its variants is to provide both an estimate of the state and its uncertainty (in the case of 3DVAR and OI, the uncertainty is assumed to be static, in 4DVAR it is implicitly evolved, but unavailable). The state’s joint probability density function (pdf) describes both of these elements, specifying the most likely value (e.g. the mean), in addition to its uncertainty (e.g. spread). Numerically evolving the pdf is an intractable task, and thus the pdf is commonly modeled as a Gaussian. If the initial state pdf is Gaussian and the dynamic model is linear, the evolution of the state distribution will remain Gaussian, with evolving mean and covariance.

The KF algorithm consists of two components: a least-squares analysis step and a propagation step using a forward model. The analysis step uses the weighted least-squares analysis estimate from OI (\ref{OI_1} and \ref{OI_2}) generalized to include an update to the state uncertainty estimate, rather than relying on the assumption of a static uncertainty estimate. The propagation step assumes a linear forward model to advance both the state and error covariance estimate to the next time when observations are available. The KF algorithm is recursive since it uses a first-guess analysis and error estimate that was produced during the propagation of an earlier state and error covariance estimate, rather than recomputing previous state estimates at every analysis time. Equations describing the analysis and propagation steps are provided below, in matrix notation. The matrix dimensions are provided in parenthesis, with \(m\) denoting the number of observations, and \(n\) denoting the number of state points.

\noindent Propagation step:
\begin{equation}
   x_k^f = Mx_{k-1}^a
\label{OI1}
\end{equation}
\begin{equation}
\label{coveqn1}
   P_k^f = MP_{k-1}^{a}M^T + Q
\end{equation}

\noindent Analysis step:
\begin{equation}
\label{OI2}
   x_k^a = x_k^f + K(y_k^o - Hx^f)
\end{equation}
\begin{equation}
\label{OI3}
   K = P_k^fH^T(HP_k^fH^T + R)^{-1}
\end{equation}
\begin{equation}
\label{coveqn2}
   P_k^a = (I-KH)P_k^f
\end{equation}
\noindent where the symbols \( k \), \( a \), \( f \) represent time, analysis, and forecast, respectively. The model state is denoted by the vector \( x \) (\(\,n\,\mathrm{x}\,1\,\)), y is the observation vector (\(\,y\,\mathrm{x}\,1\,\)), \( M \) is the linear model matrix that advances a discretized state forward in time (\(\,n\,\mathrm{x}\,n\,\)), \( P \) is the covariance matrix for the state estimate (\(\,n\,\mathrm{x}\,n\,\)), \( Q \) is the model error covariance matrix (\(\,n\,\mathrm{x}\,n\,\)), \( H \) is the linear observation operator mapping the state to the observation locations (\(\,m\,\mathrm{x}\,n\,\)), \( R \) is the observational error covariance matrix (\(\,m\,\mathrm{x}\,m\,\)), and \( K \) is the Kalman gain (\(\,n\,\mathrm{x}\,m\,\)). Equations \ref{OI1}, \ref{OI2}, and \ref{OI3}  are the traditional OI equations, while equations \ref{coveqn1} and \ref{coveqn2} are the equation to propagate and update the forecast error covariance estimate. \(W\) is denoted as \(K\) and \(B\) as \(P_k^f\) in the KF equations.

Various assumptions were made in the derivation of the KF equations. The extent to which these assumptions are satisfied determines how far the analyses stray from the optimal minimum-variance solution. These assumptions are summarized below:

\begin{enumerate}
\OUsinglespace
\item Observations are unbiased.
\item The forecast model is unbiased.
\item Observation and forecast errors are uncorrelated.
\item Observation error covariances are known.
\item Forecast model error covariances are known.
\item Forecast model is linear.
\item Forward observation operator is linear.
\end{enumerate}

Obviously, many of these assumptions will not be met for the atmospheric DA problem. The atmosphere is highly non-linear, violating (6) and potentially leading to non-Gaussian error distributions.
The forward operator for observations within the present work (e.g. reflectivity) is non-linear, violating (7). Well-documented biases exist in contemporary forecast models, violating (2). For these reasons and many others, the KF update is always suboptimal. These assumptions are inherited by the extensions of the KF described below.

\subsection{Extended Kalman filter}
The use of a linear model in the KF is a limitation for some applications, thus an extension was developed termed the extended Kalman filter (EKF; \citealt{jazwinski70}). The EKF is a modified version of the KF algorithm that implements a fully non-linear model during the state propagation step, although the linearized model is still used for the propagation of the error covariance estimate. For the update step, the background observation estimate, \(H(x^f)\), is computed with the nonlinear forward operator. The linearized forward operator remains in the Kalman gain and the covariance update.

\subsection{Ensemble Kalman filter}
Calculation of the error covariance within the KF or EKF is costly for high-dimensional problems, such as those encountered in atmospheric applications. For example, the covariance evolution requires \(2*n\) applications of the model, where \(n\) is the number of degrees of freedom of the model (i.e. size of the model). Instead of evolving the entire error covariance, a {\it Monte Carlo} simplication to the EKF was proposed by \citet{evensen94} for oceanographic applications, and subsequently applied to the atmospheric state estimation problem by \citet{houtekamermitchell98}. This ensemble Kalman filter (EnKF) uses the fully nonlinear forward model to advance an ensemble of states that consists of a random sample of the time-evolving pdf. This ensemble propagation step replaces the covariance propagation step in \ref{coveqn1}, simplifying the computation; the covariance is computed using the ensemble at each assimilation interval. 

Two types of EnKF update algorithms exist: a stochastic and a deterministic algorithm. In the stochastic algorithm, the observations dataset is perturbed with random noise prior to assimilation into each ensemble member \citep{houtekamermitchell98,hamillsnyder02}. Thus, each member assimilates a different set of observations. In the deterministic algorithm, the observations, without perturbations, are used to first update the ensemble mean, then the ensemble is updated in a manner that produces the same analysis-error covariance as the KF. Various deterministic algorithms exist; the differences between them are summarized by \citet{tippett03}. The ensemble adjustment Kalman filter, a deterministic algorithm, is used in this work and is described in the following section. Most recent studies use deterministic algorithms due to the additional source of sampling error that is introduced by stochastic algorithms \citep{whitakerhamill02}.

The EnKF has several desirable properties that should give it an advantage over 3DVAR, 4DVAR, and the EKF. First, the need to develop linearized models, both for the forward dynamics and observation operator, is removed. The cost of advancing the ensemble to obtain the covariance estimate is drastically reduced compared to the EKF. Unlike 3DVAR, the EnKF provides a fully anisotropic estimate of the background-error covariances. This permits observations to adjust the state in a manner consistent with the underlying atmospheric dynamics and patterns of error growth in the model, which are often produce complex covariance structures that are flow and time dependent \citep{bouttier94}. For example, observations in the vicinity of mesoscale surface boundaries can exhibit anistoropic error structures (Fig. \ref{cov1}), and covariance structures within convection are expected to be highly anisotropic (Fig. \ref{cov2}). However, the ensemble covariance estimates are affected by both sampling and model error. Various methods exist to reduce the impact of poor covariance estimates, including localization and inflation; a more in-depth discussion of these techniques will be included in Part I and Part II. Finally, the sample of the forecast pdf provided by the EnKF is a natural starting point for ensemble forecasting, thus the EnKF unifies the goals of data assimilation and ensemble forecasting.

\begin{figure}
\centering
\includegraphics[scale=0.6]{test}
\caption{Correlation of 2-m temperature field with 2-m temperature observation located at black dot. Adapted from \citet{knopfmeierstensrud13}. }
\label{cov1}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.75]{test2}
\caption{Ensemble mean liquid-water potential temperature (shaded) and correlation (solid contours, positive; dashed contours, negative) with a radial velocity observation located at the base of the updraft. Adapted from \citet{snyderzhang03}. }
\label{cov2}
\end{figure}

\subsection{Ensemble Adjustment Kalman filter}
\label{eakf}
The ensemble adjustment Kalman filter (EAKF) is a deterministic EnKF update algorithm proposed by \citet{anderson01}. It is mathematically equivalent to the square-root filter (EnSRF) described by \citet{whitakerhamill02}. At each data assimilation interval, the ensemble mean and members are updated using the following equations:
\begin{equation}
   \overline{x_k^a} = \overline{x_k^f} + K(y_k^o - \overline{H(x^f)})
\end{equation}
\begin{equation}
   \overline{x'^a} = \overline{x'^f} + \hat{K}\overline{H(x^f)}
\end{equation}
\begin{equation}
   \hat{K} = (1 + \sqrt{\frac{R}{HP^fH^T + R}})^{-1}K
\end{equation}

Here, \( \hat{K} \) is a reduced Kalman gain that results in smaller deviations from the ensemble mean (i.e. larger spread) compared to the full Kalman gain. Using the full Kalman gain to update each ensemble member results in excess spread reduction; in the stochastic filter the use of perturbed observations counteracts this effect. 
	
The ensemble forecast mean and forecast error covariance in the above equations are computed with the ensemble estimate using the following equations:

\begin{equation}
   P^fH^T = \frac{1}{N-1}\sum_{n=1}^{N}(x_n^f - \overline{x^f})\,[\,H(x_n^f) - \overline{H(x^f)}\,]^2
\end{equation}
\begin{equation}
   HP^fH^T = \frac{1}{N-1}\sum_{n=1}^{N}\,[\,H(x_n^f) - \overline{H(x^f)}\,]^2
\end{equation}

Assuming the observations have independent errors uncorrelated with the background forecast, each observation can be assimilated sequentially, producing the same analysis as if all observations were assimilated simulataneously. An advantage of this method is that for each observation, \( HP^fH^T \) and \( R \) become scalars, simplifying the computation of the Kalman gain (and thus the reduced Kalman gain) and preventing the computation of a potentially large covariance matrix. This simplication is employed in the EAKF algorithm.

\pagebreak
\section{Convective-scale state estimation}
Many of the DA methods reviewed in the previous section have been employed to estimate the three-dimensional state on the convective-scale \citep{sun05}. All of these methods rely on observations from weather radar, which provide the only real source of regularly available observations within convection, at the temporal and spatial scales needed to sample convective-scale structures. The two primary observation types: reflectivity and radial velocity, are directly related to the microphysical content and wind velocity within a radar volume, and thus are indirect measurements of most quantities that are useful for diagnosing storm-scale processes or initializing a numerical model (e.g. temperature, pressure, wind, microphysical content, etc.). Thus, assimilation methods must infer the quantities of interest using additional information. DA techniques, combined with an NWP model, are used to provide these missing relationships and construct a three-dimensional convective-scale analysis.

\subsection{Experiment design}
Two types of DA experiments are presented within the dissertation: observing system simulation experiments and real-data experiments. The differences and design of each type of experiment will be discussed below, in addition to choices in the formulation of the mesoscale environment.

\subsubsection{Observing system simulation experiments}
An observing system simulation experiment (OSSE) is a type of DA experiment to completely isolate the effects of the DA system on the state estimation procedure. First, a simulation is performed using a dynamic model from a given initial state in order to create the weather event of interest. Simulated observations are extracted from this “truth” simulation, with errors added to each observation that are typically drawn from a Gaussian distribution with zero mean and a prescribed standard deviation. In an OSSE, the observation locations can be completely determined by the experimenter, thus OSSEs are useful for testing the impact on analysis quality of new types and distributions of observations that may not be available at the time of the experiment. If the EnKF is chosen as the DA scheme for an OSSE, an initial ensemble needs to be constructed that samples the typical environment uncertainty. This is regularly achieved by drawing perturbations from a Gaussian and adding each realization to the truth state.

Once the truth simulation is performed and the synthetic observations are harvested, the experiment proceeds similar to a standard DA experiment. The synthetic observations are assimilated back into the model in an attempt to reconstruct the truth state. This approach assumes a “perfect” numerical model, although an “imperfect” model experiment can be designed by using a model with different characteristics (e.g. resolution, domain size, physics parameterization) during the assimilation step. The primary advantage of conducting an OSSE is that a direct comparison to the true state can be made. That is, the accuracy of the analyses due to observing system or DA changes can be isolated and quantitatively assessed.

\subsubsection{Real-data experiments}
Real-data experiments assimilate observations to estimate the true state of the atmosphere. In real-data assimilation experiments, the accuracy of the state estimate is determined through comparison with observations, rather than the truth state in OSSEs. This gives a restricted assessment of analysis accuracy, given that observations are far less plentiful than model grid points, and model error often has a significant role. Analysis quality is best assessed with observations that are not assimilated, although independent observation datasets are typically hard to obtain, given the competing desire to assimilate as many observations as possible. In these cases, observations are compared to short-term forecasts to assess analysis accuracy, given the relationship between analysis and forecast errors.

\subsubsection{Choice of mesoscale environment}
In many EnKF OSSEs, and in some real-data experiments, that study convective-scale phenomena, a horizontally homogeneous environment is used within each ensemble member (e.g. \citealt{aksoyetal09,dawsonetal12}). This approach permits a cleaner interpretation of the impact of the environment on the observed storm evolution through the use of sensitivity studies. Further, the interplay between the convection and its environment can be assessed, since a homogeneous environment is steady-state during the course of a simulation. Yet, to produce accurate forecasts of convection, convection must develop and evolve within a realistic convective environment. For example, forecasts of the 4 May 2007 Greensburg, Kansas, tornadic supercell were substantially improved when using a realistic inhomogeneous mesoscale environment in the experiments of \citet{stensrudgao10}. For convection that is associated with surface boundaries or due to synoptic-scale forcing, an inhomogeneous environment is vital. In many studies of isolated convection (e.g. a single supercell) over a short-time period, using a homogeneous environment is better justified, since the environment may not change substantially during the life cycle of the storm.

\subsection{Previous convective-scale DA studies}

\subsubsection{Retrieval algorithms using Doppler velocity}
Early approaches to the convective-scale state estimation problem used algorithms that combined radar observations with physical laws, such as the mass continuity equation, to generate three-dimensional analyses of convection \citep{sun05}. Since radial velocity is related to the wind field through a simple geometric relationship, the three-dimensional wind field can be diagnosed using radial velocity observations from at least two independent radars, combined with the mass conservation equation (so called “dual-Doppler” techniques, as first described by \citet{armijo69}, and later used by \citet{rayetal75}, \citet{brandes77}, and a lengthy list of other studies). The vertical velocity field is prone to large errors due to biased estimates of horizontal divergence that contaminates retrievals \citep{obrien70,gaoetal99}. Nevertheless, many studies continue to rely on dual-Doppler techniques to provide three-dimensional wind analyses within convection (e.g. \citealt{wurmanetal07,markowskietal08}).

Deducing a mass field that is consistent with the wind field is more difficult, yet methods were developed to retrieve temperature by utilizing the equations of motion and the three-dimensional wind field at multiple times \citep{galchen78, hanescott78, haneetal81, roux85}. Combined, the multiple-Doppler wind analyses and mass field retrieval techniques were met with some success, especially in the determination of the wind field, but also in deducing the temperature field, for example, by determining warm anomalies in thunderstorm updrafts \citep{brandes84}. On the storm scale, the retrieval of the microphysical variables is also of importance (e.g. \citealt{ziegler85,ziegler88}).

\subsubsection{Assimilating weather radar observations with 3DVAR and 4DVAR}
Given the deficiencies of the retrieval algorithms (see summaries in \citealt{shapiromewes99}, \citealt{gaoetal99}, and references therein), later work implemented fully variational approaches to retrieve the wind and thermodynamic fields. In a variational approach, the constraints that compose the cost function can be weakly imposed, (i.e. approximately, in a least-squares sense), or strongly imposed (i.e. exactly satisfied). \citet{shapiromewes99} used a cost function with radial wind observations and mass conservation constraints as both strong and weak constraints. \citet{gaoetal99} introduced a variational analysis scheme with a cost function that included observational, background, mass conservation, and smoothness constraints. \citet{gaoetal04} incorporated a background error covariance matrix within the cost function, instead of relying on penalty coefficients as in \citet{gaoetal99}.

The goal of the studies discussed so far in this section was primarily to construct gridded analyses that can be used to understand the structure and salient dynamical processes acting within convection, and not to initialize NWP models. As computational power increased and radar observations became abundant over the United States with the deployment of the WSR-88D network, the explicit prediction of convection became a reality \citep{lilly90}. \citet{linetal93} initialized a cloud model with retrieved dual-Doppler winds, thermodynamic, and microphysical fields to predict deep convection. The limited availability of dual-Doppler WSR-88D observations are not widely available, motivated studies that retrieved the three-dimensional wind field from a time-series of single-Doppler data (e.g. \citealt{shapiroetal95,gaoetal01}). Single-Doppler studies, when combined with thermodynamic retrievals, have also been used in an attempt to initialize models for features such as dry microbursts \citep{crooktuttle94}, and when provided with an initial moisture field, to initialize convection (e.g. \citealt{weygandtetal02a}).

Using the results of many of these previous studies (e.g. \citealt{shapiroetal95,gaoetal01}), a DA system was developed within the Advanced Regional Prediction System (ARPS) model \citep{xueetal95,xueetal00,xueetal01} for the explicit prediction of convection. An early version of this system, the ARPS Data Analysis System (ADAS; \citealt{brewster96}), used a successive corrections scheme \citet{bratseth86} to assimilate Doppler radial velocity observations. In addition, ADAS incorporated a cloud analysis scheme \citep{albersetal96,zhangetal98} to adjust the moisture and latent heating profiles using reflectivity observations. Using ADAS, \citet{xueetal03} assimilated WSR-88D radial wind and reflectivity observations to initialize a forecast of the 28 March 2000 tornadic supercell event that occurred in Fort Worth, TX. \citet{dawsonxue06} performed similar work, but for a severe MCS event. \citet{huetal06a,huetal06b} generated improved forecasts of this event using the \citealt{gaoetal04} 3DVAR analysis scheme and an enhanced cloud analysis scheme \citep{brewster02}. This DA system has been the foundation for initializing other post-event experiments (e.g. \citealt{huetal07,schenkmanetal11,xueetal13}) and real-time forecasts generated during the NOAA Hazardous Weather Testbed Spring Experiments (e.g. \citealt{xueetal08}). Other 3DVAR-based systems, such as the WRF-3DVAR system, have also been developed to initialize convective-scale forecasts (e.g. \citealt{barkeretal04,xiaosun07,zhaoetal08}). These forecasts can reduce the model spin-up period and improve precipitation forecasts in the 0-12 hour forecast period \citep{kainetal10,sunetal12}.

The 4DVAR technique is well suited to the objective of model initialization due to its use of a dynamical model. As discussed earlier, 4DVAR produces a model state by fitting the state to a time series of observations using both the dynamical model and its adjoint. This retrieves all unobserved fields simultaneously, producing a dynamically consistent three-dimensional state, including wind, temperature, pressure, and microphysical fields that can be naturally used to initialize a numerical model. The retrieval techniques discussed in this section construct a wind field first, and use it to retrieve thermodynamic fields. \citet{sunetal91} and \citet{suncrook94} applied 4DVAR to develop a variational Doppler radar assimilation system (VDRAS) to retrieve the wind and thermodynamic fields using single-Doppler velocity data. \citet{suncrook97} and \citet{suncrook98}, used VDRAS to produce analyses of moist convection, both in an OSSE and with real observations. \citet{wuetal00} furthered these early results. More recently, a 4DVAR system has been implemented with the WRF model \citep{wangetal13}. In 4DVAR, the development of the adjoint model, especially with the microphysical parameterizations needed for deep convection, is both costly to develop and computationally expensive, thus other approaches have been sought.

\subsubsection{Assimilating weather radar observations with the EnKF}
\label{background_radarenkf}
The EnKF has been proposed as an alternate assimilation strategy to initialize the three-dimensional state associated with convection. An extensive amount of recent literature has devoted attention to assimilating weather radar observations, both Doppler radial velocity and reflectivity, using the EnKF, beginning with the initial OSSE study of \citet{snyderzhang03}. Others have used OSSEs to examine various fundamental challenges with radar DA using the EnKF (e.g. \citealt{zhangetal04,tongxue05,cayaetal05,xueetal06,jungetal08a,xuetal08,luxu09,thompsonetal12}). Alongside these developments, others have experimented with the assimilation of research or operational radar datasets (e.g. \citealt{dowelletal04,aksoyetal09,dowellwicker09,dowelletal11,snooketal11,dawsonetal12,jungetal12,tanamachietal13,yussoufetal13}).

These studies have assimilated Doppler radial velocity and/or reflectivity observations. While radial velocity is directly related to the three-dimensional wind field, radar reflectivity depends on the specifics of the hydrometeor distribution, including number, size, phase, and orientation. Predictions of the evolution of hydrometeor distributions are parameterized, thus described by analytical functions that evolve using uncertain estimates of precipitation process tendencies, leading to significant model error. Additional errors exist due to uncertainties in the computation of reflectivity from the microphysical variables in the model state (i.e. forward operator errors). For example, the increase in the sampling volume of the radar beam with range is not usually taken into account in the forward operator. Further, the relationship between reflectivity and the model state is non-linear, violating the conditions for optimality in the EnKF and leading to a suboptimal update. Finally, reflectivity observation errors can be large due to miscalibration of the radar system and signal attenuation. Together, these errors sources present a formidable challenge to assimilating radar reflectivity.

To isolate the effects of reflectivity assimilation on storm-scale analyses, \citet{dowelletal11} conducted sensitivity tests using radar observations of an observed supercell. While reflectivity observations helped to accelerate the spin-up of convection in the analyses (especially at far distances from the radar), biases contributed to large errors in within the storm analyses. They attributed these errors to be predominately from two sources: 1) the reflectivity forward operator, especially in the computation of reflectivity contributions from ice and mixed-phase precipitation, and 2) microphysical parameterization scheme reflectivity bias. Errors from the second source were noted within the downshear anvil at low-levels, hypothesized to be due to errors in the model’s prediction of rainwater mixing ratio within the supercell forward-flank. Persistent, unnecessary, adjustments to temperature, via cross-correlations between reflectivity and temperature, played a role in producing forward-flank cold pools that were too intense. Limiting the DA update to the hydrometeor fields ameliorated this bias, at the expense of a longer spin-up period. These biases are common in single-moment microphysics schemes of the type used in \citet{dowelletal11}, and may be reduced by using schemes that predict more moments of the hydrometeor distribution, such as number concentration or reflectivity \citep{dawsonetal10}.

\subsubsection{Ensemble forecasts of convection using the EnKF}
Only a small number of studies have analyzed short-term forecasts of convective systems with initial states generated by the EnKF. \citet{aksoyetal10} used an EnKF ensemble to initialize a 30-minute forecast of three different convective events (a supercell, multi-cell, and squall line). Their experimental design used a homogeneous environment without parameterization of boundary layer or surface processes. They surmised that these simplifications led to the some of the ensemble forecast errors, including inaccurate predictions of storm maintenance due to environments that did not support convection. They noted that these results demonstrated the “importance of incorporating mesoscale information, both in terms of the environment and its uncertainty, for successful convective-scale data assimilation and forecasting.” 

Even in a relatively spatially homogeneous environment, such as that associated with an isolated supercell, important temporal changes in the wind and temperature profiles can have marked effects on short-term forecasts. These changes are often dramatic over the lifecycle of a convective storm, as the loss of daytime heating causes changes to the low-level temperature and wind profiles (e.g. low-level jet) that can modulate storm motion, intensity, and severity. \citet{dawsonetal12} produced 1-hour forecasts of a tornadic, isolated supercell, in an environment characterized by a rapidly strengthening low-level jet. Various low-level wind profiles, taken over the course of a one-hour period, were used to initialize a homogeneous environment. Forecasts of the area impacted by significant low-level rotation within the supercell were quite sensitive to the chosen low-level wind profile, even over this short time period. They concluded by noting that initializing forecasts in these types of environments is “problematic”.
	
While \citet{aksoyetal10} and \citet{dawsonetal12} produced 1-hr ensemble forecasts from an EnKF system, \citet{snooketal12} assimilated WSR-88D and CASA radar data using the EnKF and produced 3-hr ensemble forecasts for an MCS containing tornadic mesovortices. \citet{snooketal12} focused primarily on sensitivities of the forecasts of mesovortices to microphysical parameterization and the assimilation of high-resolution CASA radar data.

\subsubsection{Some challenges for successful radar DA}
Much of the existing literature has focused on isolated convective events, oftentimes a single, confined area of convection such as an isolated supercell. Radar DA techniques, especially those involving the ensemble Kalman filter (EnKF) have matured by studying these events almost exclusively. For the more complex convective event discussed herein, challenges remain with the successful assimilation of radar data, including both reflectivity and radial velocity.

One successful assimilation strategy that has been used with previous studies is to use additive noise to promote the rapid spin-up of convection and maintain ensemble spread \citep{dowellwicker09}. To achieve the former objective, the magnitudes of noise added to the analyses are often the largest at the beginning of the assimilation period, and then are reduced to a smaller magnitude later in the period to achieve the latter goal. Even though the additive noise procedure can exacerbate temperature errors within the surface cold pool and is largely an ad hoc technique, it is regarded as having an overall positive benefit on the convective analyses \citep{dowelletal11} and is used extensively in storm-scale EnKF radar DA studies.

A similar strategy is used with the assimilation of reflectivity observations; these observations promote the rapid spin-up of convection, but are often detrimental to the analyses after a spin-up period. Thus, reflectivity assimilation is often ended after an initial predetermined time period, with later assimilation cycles assimilating only radial velocity and clear-air observations. It is unclear how these two assimilation strategies can be generalized to a more complex convective event such as the one studied herein. In this case, multiple areas of convection are developing at different times through the assimilation period. Turning off reflectivity assimilation or reducing additive noise at a prescribed time over the entire domain would potentially hinder the development of later convection, and be detrimental to convection that has already spun-up prior to the cut-off time.

A secondary challenge is the use of clear-air radar observations (i.e. a subset of reflectivity observations less than a predetermined threshold, usually around 10 dBZ) that serve to prevent spurious convection during the assimilation period. In general, these observations have been assimilated following the initial development of convection. This is a natural consequence of initializing experiments near or slightly following the time of CI, when reflectivity and radial velocity observations first become available. In the present case, CI is observed in different times and regions within the domain. Using the traditional approach of assimilating clear-air observations when precipitation first develops within the domain may adjust the environment toward a state that is unfavorable for convection in areas where convection may later develop. In these regions, the adjustments that take place may have to be reversed when reflectivity and radial velocity observations within precipitation become available. The ramifications of this process, and thus the most effective use of clear-air observations, remain largely unknown.

\section{Mesoscale state estimation at the surface}
Observations near the surface (e.g. temperature, dew point temperature, horizontal wind components) have the potential to be utilized in DA systems to provide more accurate estimates of the temperature and moisture profiles within the planetary boundary layer (PBL), as well as the placement and strength of near surface boundaries (e.g. fronts, dry lines). These surface boundaries often serve to focus the development of deep convection. Once convection has developed, the resulting mode, intensity, and coverage of convection is strongly controlled by the temperature and moisture profile within the PBL. Thus it is a desirable goal to improve estimates of the structure, placement, and horizontal and vertical circulations associated with these boundaries, in addition to the representations of the PBL. These improvements should lead to more accurate forecasts of CI and its subsequent evolution. Efforts to assimilate surface observations in numerical models, with specific emphasis on improving forecasts of convection, are summarized in this section.

\subsection{Assimilating surface observations into NWP models}
Many surface observation networks exist that provide high-resolution temporal and spatial data (e.g. Oklahoma Mesonet, West Texas Mesonet). These networks provide a rich data source that can be assimilated into numerical forecast models. So far, these data have been underutilized operationally due to challenges with their assimilation \citep{puetal13}. For example, mismatches often occur between the model and surface observations in areas of terrain, where, due to model resolution, the model surface may not be at the same height as the surface observation height. Also, surface observations are not always coupled to the overlying atmosphere, thus their impact on the column above the observation location is difficult to prescribe and control. Further, the parameterized PBL and surface layers are often biased in environments conducive to the development of deep convection \citep{coniglioetal13}, leading to suboptimal, and potentially detrimental, assimilation performance. 

Even in light of these challenges, recent studies have demonstrated some success in assimilating surface observations on the mesoscale using the EnKF. Using perfect and imperfect model experiments, \citet{zhangetal06a} and \citet{mengzhang07} showed that assimilating surface observations every 3-hours improved analyses and forecasts of a poorly forecast snowstorm. \citet{mengzhang08a} extended the previous two studies by assimilating real surface and rawinsonde data during a mesoscale convective vortex event. In \citet{mengzhang08a}, surface observations were assimilated every 6-hours, and reduced forecast error, although surface observations had a much smaller impact compared to sounding and profiler data, and thus were not assimilated in a follow-up study \citep{mengzhang08b}. In these studies, the model’s horizontal grid spacing was 30 km. Surface observations were placed every 60 km (in \citealt{zhangetal06a} and \citealt{mengzhang07}) or where routine\footnote{Routine surface observations include METAR and exclude observations from mesonetworks (e.g. Oklahoma mesonet).} surface observations were available (in \citealt{mengzhang08a}). Other studies have investigated the use of the EnKF for assimilating surface observations over complex terrain \citep{puetal13,ancelletal11}. Again, routine observations were used in these studies on model grids ranging from 12-km \citep{ancelletal11} to 27-km \citep{puetal13}.

Other studies have focused more closely on how to effective assimilate surface observations and their ability to identify and reduce model errors within a parameterized planetary boundary layer (PPBL). Using a perfect and imperfect single-column model and simulated observations, \citet{hackersnyder05} demonstrated that assimilating surface observations, using the EnKF, constrained the entire profile within the PPBL, especially when the surface was deeply coupled with the atmosphere (e.g. during the late afternoon when the PPBL was well-mixed). The height of the PPBL was also improved when surface observations were assimilated. \citet{hackeredelstein07} arrived at similar conclusions using real observations over Oklahoma.

\subsection{Assimilating surface observations: Convective weather applications}
Several studies have focused more directly on the benefits of surface observations on mesoscale predictions of severe weather. \citet{fujitaetal07} exclusively assimilated surface observations each hour onto a 30-km model grid during the 12 UTC - 18 UTC period preceding two severe weather events. An 18-hour ensemble forecast was initialized at the end of each assimilation period. The authors noted improvements in the placement and intensity of mesoscale boundaries such as dry lines and fronts, in PBL depth and structure, and in resulting placement of precipitation both in the analyses and throughout the ensemble forecast period. \citet{stensrudetal09b} demonstrated improved analyses of two mesoscale convective system (MCS) events by assimilating hourly surface temperature, moisture, and wind observations. \citet{wheatleystensrud10} expanded on the \citet{stensrudetal09b} study by isolating the potential positive benefit of assimilating total surface pressure (via altimeter) observations on analyses of MCS cold pools, rather than the assimilation of 1-hour surface pressure tendency. \citet{wheatleyetal12} assimilated surface observations, along with radiosonde, marine, and aircraft observations, each hour during a 5-hour period and produced improved short-term forecasts of severe weather parameters in 0-12-hr ensemble forecasts. The specific impact of surface observations in \citet{wheatleyetal12} is unknown. Routinely available surface observations were assimilated in all of the previous studies mentioned so far. In addition to routine observations, \citet{knopfmeierstensrud13} assimilated mesonet observations every hour for two two-week periods and, through data denial experiments, determined that the mesonet observations did not have a significant impact on the resulting analyses. They suggest that a clearer benefit may result by using more frequent assimilation cycles.

Several studies have noted that assimilating surface observations might be a boon to forecasts of CI. For example, while discussing the beneficial impact of surface data on boundary layer profiles in their idealized results, \citet{hackersnyder05} commented that the {\it "extension of these results to real modeling systems could improve forecasts of pollutant transport and storm initiation."} Yet, few studies have focused specifically on the effects of surface DA on short-term forecasts of CI and convective evolution. Previous work has focused primarily on improved representation of the environments using grid spacings of 12 km or greater. At these grid lengths, convective parameterization is required, thus the benefits to convection initiation and evolution cannot be explicitly addressed, but only inferred through the convective precipitation output from the parameterization scheme. For example, \citet{ancell12}, surface observations were assimilated every 3-hours on a 4-km grid, while \citet{knopfmeierstensrud13} used a 5-km grid, but neither of these studies focused on convective development (in \citealt{ancell12} the model domain covered only portions of the Pacific Northwest). Further, with grid spacings ranging from 12 km to 30 km, the model grids only are able to resolve features with spatial scales within the meso-alpha scale (100-1000 km). Smaller scale features that often play a role in initiating convection (e.g. boundary layer rolls, smaller outflow boundaries) are left unresolved.

Observations gathered during the International \(H_2O\) project (IHOP; \citealt{weckwerthetal04}) spurred a collection of studies focused on understanding the key processes involved in CI (e.g. \citealt{weckwerthparsons06}) and predicting CI using NWP models with convection-permitting grid spacing (e.g. \cite{xuemartin06a,xuemartin06b}). While most of CI modeling studies assimilated surface observations, including special observation sets collected during IHOP, their goals differed. \citet{xuemartin06a}, hereafter XM06a, focused on the skill of short-term model forecasts (3-hr lead time) at predicting CI locations for the 24 May 2002 IHOP case. XM06a used an initial analysis produced by assimilating routine and special IHOP upper-air and surface observations with ADAS \citep{brewster96}. Although observations were assimilated at only two times (12 UTC and 18 UTC), the deterministic forecast produced a fairly accurate prediction of CI and the general evolution of the convective line. Other studies focused on identifying the physical mechanisms leading to CI in simulations similar to those produced in XM06a \citep{xuemartin06b}, hereafter XM06b, and \citet{wangxue12}. None of these studies systematically investigated the impact of the various observations types or assimilation strategies on the resulting analyses and forecasts. 

\citet{liuxue08}, hereafter LX08, followed a similar approach to XM06a and XM06b to study the IHOP case on 12 June 2012, but performed an additional set of sensitivity experiments to examine how varying DA intervals, correlation length scales, and nonstandard observations types affect predictions of CI and evolution. The timing and placement of multiple areas of CI in the various experiments were sensitive to these choices. An experiment with 3-hourly assimilation cycles during a 6-hour period provided a better overall forecast of CI timing, while an experiment with hourly assimilation provided a better forecast of convective evolution. Yet, the hourly assimilation experiments improved the forecast of the low-level wind field, compared to a 3-hour and 6-hour assimilation period. LX08 suggested that the delay in CI observed in some of their sensitivity experiments was due, in part, to the weakening of resolved low-level horizontal convergence after each assimilation cycle, since the assimilated surface observations have insufficient spatial resolution to provide convective-scale information. Finally, an experiment with a larger vertical correlation length scale resulted in a deeper, longer-lived cold pool, leading to increased convergence along the cold pool gust front. This improved forecasts of CI initiated by the gust front, but hindered CI and evolution in other regions. 

To the best of this author’s knowledge, LX08 is the only study that has explored how surface observation datasets and assimilation strategies affect convective-scale analyses and forecasts of CI and evolution. Thus, while some of the goals of the present work are shared with the LX08 study, substantial differences exist. First, ADAS, as used in LX08, implements the \citet{bratseth86} successive correction method to produce the analyses. This system is now largely obsolete in the NWP community, replaced with more advanced DA algorithms, including state-of-the-art schemes that provide flow-dependent estimates of the forecast errors (e.g. EnKF). LX08 realized the need for such capabilities, stating, {\it "for truly optimal analysis, flow-dependent background error correlation scales have to be estimated and used."}

\section{Combined surface and radar data assimilation}
While surface observations have the potential to improve CI forecasts, their use is also important following CI, when radar observations become available. If radar observations are being assimilated in regions where the mesoscale environment is unsupportive of convection, for instance, due to errors in the placement of critical surface boundaries, then convection can fail to become established in the model and dissipate rapidly in any forecasts initialized with these analyses. Assimilating surface observations can ensure the simulated environment supports convection and improve the evolution of convection beyond the early stages of the forecast by making lasting adjustments to the mesoscale environment.

Several studies have assimilated both surface and radar data for the multi-scale prediction of convection, but most have used variational, not ensemble, approaches. For example, the ARPS ADAS and 3DVAR system have been frequently used to assimilate both observation types for the prediction of a variety of events (e.g. \citealt{dawsonxue06,huetal07,schenkmanetal11}). Since most EnKF real-data radar DA studies have used a homogeneous environment to predict isolated areas of convection (e.g. \citealt{dowelletal11,dawsonetal12}), the utility of surface observations is limited. Surface observations would only be useful within convectively-generated features, such as the surface cold pool. The lack of routinely available surface measurements on the convective-scale has precluded the use of these observations in storm-scale DA studies that employ homogeneous environments, although recently \citet{marquisetal14} used mobile mesonet observations from a field experiment to constrain the development of the surface cold pool for a supercell event. Surface observations are more useful in a heterogeneous environment, yet few examples exist where both surface and radar data were assimilated on a convection-permitting grid. Most recent work has assimilated surface observations on a coarser grid, while assimilating radar data on a finer grid (e.g. \citealt{yussoufetal13}). Further discussion of these studies and their relation to the current work is provided in Chapter 4.


